{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T22:21:27.127515Z",
     "start_time": "2020-09-11T22:21:27.107558Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "\n",
    "\n",
    "EPOCHS     = 200\n",
    "BATCH_SIZE = 1000\n",
    "VERBOSE    = 1\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()#tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "O = 'Berlin'\n",
    "\n",
    "def get_data():\n",
    "    print(f\"Loading raw data for {O}.\")\n",
    "    FILENAME =  f\"/Users/sonat/Documents/DataSets/Spatial/{O}_maps_labels.npz\"\n",
    "    data     = np.load(FILENAME)\n",
    "    labels   = data['labels']\n",
    "    data     = data[\"maps\"]\n",
    "    \n",
    "    print(f\"Loaded a dataset with {data.shape} and {labels.shape[0]} labels\")\n",
    "    return data, labels\n",
    "def flatten(data):\n",
    "    print('Flattening data')\n",
    "    print(f\"Old size:{data.shape}\")\n",
    "    data = data.reshape((data.shape[0],data.shape[1]*data.shape[2]))\n",
    "    print(f\"New size:{data.shape}\")\n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "    print(\"Normalizing data\")\n",
    "    data = data / np.std(data,axis=0)\n",
    "    if not np.any(np.isclose(np.std(data,axis=0),1)):\n",
    "        sys.exit('Normalization didn''t work')\n",
    "    return data\n",
    "\n",
    "def center(data):\n",
    "    print(\"Centering data\")\n",
    "    data = data - np.mean(data,axis=0)\n",
    "    if not np.any(np.isclose(np.mean(data,axis=0),0)):\n",
    "        sys.exit('Centering didn''t work')\n",
    "    return data\n",
    "\n",
    "def take_most_active(data):\n",
    "    return data\n",
    " \n",
    "    \n",
    "def splitter(data,labels):\n",
    "\n",
    "    print('Splitter called.')\n",
    "    X_train, X_test, Y_train, Y_test = \\\n",
    "    train_test_split(data,labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    TRAIN_SAMPLES = X_train.shape[0]\n",
    "    TEST_SAMPLES  = X_test.shape[0]\n",
    "    \n",
    "    # cast\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # One-hot representation of the labels.\n",
    "    NB_CLASSES     = np.unique(Y_train).shape[0]\n",
    "    print(NB_CLASSES)\n",
    "    Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "    Y_test  = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def two_hidden_layer(data, labels,activation,N_HIDDEN=16):\n",
    "    INPUT_SHAPE = (data.shape[1],)\n",
    "    NB_CLASSES     = np.unique(labels).shape[0]\n",
    "    print(f\"Building Network with 2 hidden layers alike\")\n",
    "    print(f\"Found {NB_CLASSES} classes\")\n",
    "    print(f\"Found {INPUT_SHAPE} input size\")\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(N_HIDDEN,input_shape=INPUT_SHAPE,name='dense_layer', activation='relu'))\n",
    "    model.add(keras.layers.Dense(N_HIDDEN,name='dense_layer_2', activation='relu'))\n",
    "    model.add(keras.layers.Dense(NB_CLASSES,name='linear', activation=activation))    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def logistic(data, labels,activation):\n",
    "    INPUT_SHAPE = (data.shape[1],)\n",
    "    NB_CLASSES     = np.unique(labels).shape[0]\n",
    "    print(f\"Building model Logistic Network alike\")\n",
    "    print(f\"Found {NB_CLASSES} classes\")\n",
    "    print(f\"Found {INPUT_SHAPE} input size\")\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                                 input_shape=INPUT_SHAPE,\n",
    "                                 name='dense_layer', \n",
    "                                 activation=activation))\n",
    "    \n",
    "    return model\n",
    "\n",
    "pipeline = { \n",
    "#             0: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,         \n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : logistic,\n",
    "#                    'model_args'   : {'activation':'sigmoid'}\n",
    "#                 },\n",
    "#             1: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,         \n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : logistic,\n",
    "#                    'model_args'   : {'activation':'softmax'}\n",
    "#                 },\n",
    "#             2: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,\n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : two_hidden_layer,\n",
    "#                    'model_args'   : {'activation':'softmax','N_HIDDEN':16}\n",
    "#                 },\n",
    "#             3: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,\n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : two_hidden_layer,\n",
    "#                    'model_args'   : {'activation':'softmax','N_HIDDEN':64}\n",
    "#                 },\n",
    "#             4: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,\n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : two_hidden_layer,\n",
    "#                    'model_args'   : {'activation':'softmax','N_HIDDEN':8}\n",
    "#                 },\n",
    "#             5: #one-layer, ie logistric regression\n",
    "#                   {'data_loader'  : get_data,\n",
    "#                    'preprocessors': [flatten, normalize],\n",
    "#                    'splitter'     : splitter,\n",
    "#                    'model'        : two_hidden_layer,\n",
    "#                    'model_args'   : {'activation':'softmax','N_HIDDEN':64}\n",
    "#                 },\n",
    "            6: #one-layer, ie logistric regression\n",
    "                  {'data_loader'  : get_data,\n",
    "                   'preprocessors': [flatten, center],\n",
    "                   'splitter'     : splitter,\n",
    "                   'model'        : two_hidden_layer,\n",
    "                   'model_args'   : {'activation':'sigmoid','N_HIDDEN':64},\n",
    "                   'optimizer'    : tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "                },\n",
    "            7: #one-layer, ie logistric regression\n",
    "              {'data_loader'  : get_data,\n",
    "               'preprocessors': [flatten, center],\n",
    "               'splitter'     : splitter,\n",
    "               'model'        : two_hidden_layer,\n",
    "               'model_args'   : {'activation':'sigmoid','N_HIDDEN':64},\n",
    "               'optimizer'    : tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "                },\n",
    "            8: #one-layer, ie logistric regression\n",
    "              {'data_loader'  : get_data,\n",
    "               'preprocessors': [flatten, center],\n",
    "               'splitter'     : splitter,\n",
    "               'model'        : two_hidden_layer,\n",
    "               'model_args'   : {'activation':'sigmoid','N_HIDDEN':64},\n",
    "               'optimizer'    : tf.keras.optimizers.Adam(learning_rate=1)\n",
    "                },\n",
    "            }\n",
    "#                  {'model_fun':logistic,'data_fun': get_data,'preprocess_fun': preprop}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T22:22:50.306105Z",
     "start_time": "2020-09-11T22:21:27.652751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: /tmp/tfdbg2_logdir, tensor debug mode: FULL_HEALTH)\n",
      "Loading raw data for Berlin.\n",
      "Loaded a dataset with (21980, 30, 30) and 21980 labels\n",
      "Flattening data\n",
      "Old size:(21980, 30, 30)\n",
      "New size:(21980, 900)\n",
      "Centering data\n",
      "Splitter called.\n",
      "17584 train samples\n",
      "4396 test samples\n",
      "7\n",
      "Building Network with 2 hidden layers alike\n",
      "Found 7 classes\n",
      "Found (900,) input size\n",
      "WARNING:tensorflow:Failed to read source code from path: /Users/sonat/Documents/repos/MachineLearningLecture/<ipython-input-11-de79ef785de2>. Reason: Source path neither exists nor can be loaded as a .par file: /Users/sonat/Documents/repos/MachineLearningLecture/<ipython-input-11-de79ef785de2>\n",
      "WARNING:tensorflow:Failed to read source code from path: /Users/sonat/Documents/repos/MachineLearningLecture/<ipython-input-10-0975fad9a96d>. Reason: Source path neither exists nor can be loaded as a .par file: /Users/sonat/Documents/repos/MachineLearningLecture/<ipython-input-10-0975fad9a96d>\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 64)                57664     \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "linear (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 62,279\n",
      "Trainable params: 62,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      " 2/15 [===>..........................] - ETA: 0s - loss: 2.8320 - accuracy: 0.1425WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.1012s). Check your callbacks.\n",
      "15/15 [==============================] - 1s 52ms/step - loss: 2.0714 - accuracy: 0.1349 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s 14ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.9459 - accuracy: 0.1363 - val_loss: 1.9459 - val_accuracy: 0.1390\n",
      "138/138 [==============================] - 0s 3ms/step - loss: 1.9459 - accuracy: 0.1317\n",
      "Test accuracy: 0.13171064853668213\n",
      "Loading raw data for Berlin.\n",
      "Loaded a dataset with (21980, 30, 30) and 21980 labels\n",
      "Flattening data\n",
      "Old size:(21980, 30, 30)\n",
      "New size:(21980, 900)\n",
      "Centering data\n",
      "Splitter called.\n",
      "17584 train samples\n",
      "4396 test samples\n",
      "7\n",
      "Building Network with 2 hidden layers alike\n",
      "Found 7 classes\n",
      "Found (900,) input size\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 64)                57664     \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "linear (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 62,279\n",
      "Trainable params: 62,279\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      " 2/15 [===>..........................] - ETA: 0s - loss: 1.9966 - accuracy: 0.1365WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0079s vs `on_train_batch_end` time: 0.1381s). Check your callbacks.\n",
      "15/15 [==============================] - 1s 58ms/step - loss: 1.9298 - accuracy: 0.1750 - val_loss: 1.8783 - val_accuracy: 0.2007\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 1.8258 - accuracy: 0.2365 - val_loss: 1.8151 - val_accuracy: 0.2255\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 1.7480 - accuracy: 0.2745 - val_loss: 1.7647 - val_accuracy: 0.2707\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.6707 - accuracy: 0.3126 - val_loss: 1.7254 - val_accuracy: 0.2900\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.5948 - accuracy: 0.3483 - val_loss: 1.7089 - val_accuracy: 0.2991\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.5221 - accuracy: 0.3810 - val_loss: 1.7092 - val_accuracy: 0.3042\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.4566 - accuracy: 0.4112 - val_loss: 1.7166 - val_accuracy: 0.3054\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.3972 - accuracy: 0.4400 - val_loss: 1.7258 - val_accuracy: 0.3253\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 1.3382 - accuracy: 0.4710 - val_loss: 1.7544 - val_accuracy: 0.3170\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 1.2875 - accuracy: 0.4962 - val_loss: 1.7614 - val_accuracy: 0.3162\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 1.2378 - accuracy: 0.5177 - val_loss: 1.7983 - val_accuracy: 0.3207\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.1935 - accuracy: 0.5393 - val_loss: 1.8050 - val_accuracy: 0.3244\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 1.1522 - accuracy: 0.5601 - val_loss: 1.8339 - val_accuracy: 0.3216\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.1113 - accuracy: 0.5777 - val_loss: 1.8662 - val_accuracy: 0.3213\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 1.0741 - accuracy: 0.6014 - val_loss: 1.8844 - val_accuracy: 0.3236\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.0352 - accuracy: 0.6190 - val_loss: 1.9174 - val_accuracy: 0.3213\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 1.0049 - accuracy: 0.6318 - val_loss: 1.9497 - val_accuracy: 0.3210\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.9741 - accuracy: 0.6433 - val_loss: 1.9707 - val_accuracy: 0.3159\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.9378 - accuracy: 0.6627 - val_loss: 2.0101 - val_accuracy: 0.3273\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.9059 - accuracy: 0.6749 - val_loss: 2.0509 - val_accuracy: 0.3170\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.8789 - accuracy: 0.6875 - val_loss: 2.0953 - val_accuracy: 0.3190\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.8551 - accuracy: 0.6996 - val_loss: 2.1145 - val_accuracy: 0.3145\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.8233 - accuracy: 0.7154 - val_loss: 2.1626 - val_accuracy: 0.3113\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7947 - accuracy: 0.7263 - val_loss: 2.1879 - val_accuracy: 0.3145\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7715 - accuracy: 0.7360 - val_loss: 2.2457 - val_accuracy: 0.3031\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7445 - accuracy: 0.7488 - val_loss: 2.2666 - val_accuracy: 0.3074\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7148 - accuracy: 0.7610 - val_loss: 2.3036 - val_accuracy: 0.3111\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.6893 - accuracy: 0.7722 - val_loss: 2.3678 - val_accuracy: 0.3148\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.6591 - accuracy: 0.7858 - val_loss: 2.4122 - val_accuracy: 0.3096\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.6449 - accuracy: 0.7916 - val_loss: 2.4503 - val_accuracy: 0.3119\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6209 - accuracy: 0.8002 - val_loss: 2.5245 - val_accuracy: 0.3074\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.6062 - accuracy: 0.8082 - val_loss: 2.5239 - val_accuracy: 0.3145\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5790 - accuracy: 0.8203 - val_loss: 2.5770 - val_accuracy: 0.3113\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.5588 - accuracy: 0.8270 - val_loss: 2.6461 - val_accuracy: 0.3148\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5743 - accuracy: 0.8177 - val_loss: 2.7169 - val_accuracy: 0.3076\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.5698 - accuracy: 0.8204 - val_loss: 2.7599 - val_accuracy: 0.3125\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.5342 - accuracy: 0.8344 - val_loss: 2.7683 - val_accuracy: 0.3111\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.5038 - accuracy: 0.8504 - val_loss: 2.8072 - val_accuracy: 0.3139\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4802 - accuracy: 0.8632 - val_loss: 2.8452 - val_accuracy: 0.3176\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4749 - accuracy: 0.8612 - val_loss: 2.9101 - val_accuracy: 0.3113\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4582 - accuracy: 0.8689 - val_loss: 2.9337 - val_accuracy: 0.3085\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4347 - accuracy: 0.8788 - val_loss: 2.9785 - val_accuracy: 0.3136\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4146 - accuracy: 0.8848 - val_loss: 3.0337 - val_accuracy: 0.3045\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.4015 - accuracy: 0.8914 - val_loss: 3.0666 - val_accuracy: 0.3094\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3931 - accuracy: 0.8877 - val_loss: 3.1157 - val_accuracy: 0.3085\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3737 - accuracy: 0.9001 - val_loss: 3.1404 - val_accuracy: 0.3119\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3556 - accuracy: 0.9078 - val_loss: 3.2169 - val_accuracy: 0.3065\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3464 - accuracy: 0.9094 - val_loss: 3.2749 - val_accuracy: 0.3045\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3388 - accuracy: 0.9111 - val_loss: 3.3095 - val_accuracy: 0.3082\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.3227 - accuracy: 0.9174 - val_loss: 3.3440 - val_accuracy: 0.3136\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.3069 - accuracy: 0.9241 - val_loss: 3.3948 - val_accuracy: 0.3142\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.2987 - accuracy: 0.9242 - val_loss: 3.4570 - val_accuracy: 0.3059\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2865 - accuracy: 0.9310 - val_loss: 3.4904 - val_accuracy: 0.3048\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2763 - accuracy: 0.9323 - val_loss: 3.5054 - val_accuracy: 0.3028\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2648 - accuracy: 0.9372 - val_loss: 3.5471 - val_accuracy: 0.3111\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2521 - accuracy: 0.9411 - val_loss: 3.6080 - val_accuracy: 0.3071\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2470 - accuracy: 0.9419 - val_loss: 3.6584 - val_accuracy: 0.3014\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.2363 - accuracy: 0.9464 - val_loss: 3.7103 - val_accuracy: 0.3037\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.2367 - accuracy: 0.9443 - val_loss: 3.7687 - val_accuracy: 0.3094\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.2208 - accuracy: 0.9505 - val_loss: 3.8150 - val_accuracy: 0.3037\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.2165 - accuracy: 0.9512 - val_loss: 3.8586 - val_accuracy: 0.3082\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2122 - accuracy: 0.9512 - val_loss: 3.8888 - val_accuracy: 0.3042\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1990 - accuracy: 0.9582 - val_loss: 3.9336 - val_accuracy: 0.3082\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1909 - accuracy: 0.9594 - val_loss: 3.9595 - val_accuracy: 0.3048\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1845 - accuracy: 0.9620 - val_loss: 4.0181 - val_accuracy: 0.3042\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1782 - accuracy: 0.9635 - val_loss: 4.0631 - val_accuracy: 0.3034\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1786 - accuracy: 0.9620 - val_loss: 4.1697 - val_accuracy: 0.2974\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1779 - accuracy: 0.9613 - val_loss: 4.1726 - val_accuracy: 0.3008\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1648 - accuracy: 0.9663 - val_loss: 4.1880 - val_accuracy: 0.2974\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1544 - accuracy: 0.9713 - val_loss: 4.2810 - val_accuracy: 0.2926\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1604 - accuracy: 0.9665 - val_loss: 4.3278 - val_accuracy: 0.3040\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1591 - accuracy: 0.9658 - val_loss: nan - val_accuracy: 0.2983\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1436 - accuracy: 0.9718 - val_loss: 4.3422 - val_accuracy: 0.3062\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.1338 - accuracy: 0.9756 - val_loss: 4.4035 - val_accuracy: 0.3034\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1285 - accuracy: 0.9776 - val_loss: 4.4291 - val_accuracy: 0.3008\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.1253 - accuracy: 0.9772 - val_loss: 4.4762 - val_accuracy: 0.3071\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1285 - accuracy: 0.9767 - val_loss: 4.5374 - val_accuracy: 0.3014\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1228 - accuracy: 0.9769 - val_loss: nan - val_accuracy: 0.3020\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1126 - accuracy: 0.9817 - val_loss: 4.5981 - val_accuracy: 0.3003\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.1168 - accuracy: 0.9789 - val_loss: 4.6217 - val_accuracy: 0.3005\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.1067 - accuracy: 0.9817 - val_loss: nan - val_accuracy: 0.2980\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1055 - accuracy: 0.9820 - val_loss: 4.7182 - val_accuracy: 0.2966\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0984 - accuracy: 0.9844 - val_loss: 4.7384 - val_accuracy: 0.2991\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0945 - accuracy: 0.9849 - val_loss: nan - val_accuracy: 0.2957\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0947 - accuracy: 0.9840 - val_loss: nan - val_accuracy: 0.2971\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0910 - accuracy: 0.9854 - val_loss: nan - val_accuracy: 0.3017\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0830 - accuracy: 0.9882 - val_loss: nan - val_accuracy: 0.2971\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0842 - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.3003\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 19ms/step - loss: 0.0822 - accuracy: 0.9872 - val_loss: nan - val_accuracy: 0.2957\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0770 - accuracy: 0.9886 - val_loss: nan - val_accuracy: 0.2977\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0760 - accuracy: 0.9890 - val_loss: nan - val_accuracy: 0.2980\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.0708 - accuracy: 0.9900 - val_loss: nan - val_accuracy: 0.2949\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0729 - accuracy: 0.9895 - val_loss: nan - val_accuracy: 0.2991\n",
      "Epoch 94/200\n",
      "10/15 [===================>..........] - ETA: 0s - loss: 0.0668 - accuracy: 0.9902"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: dense_layer/kernel_0 [Op:WriteHistogramSummary]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-de79ef785de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m               callbacks=[tensorboard_callback])\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_log_weights\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m   2232\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2233\u001b[0m             \u001b[0mweight_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m             \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_weight_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(name, tensor, family, step)\u001b[0m\n\u001b[1;32m    834\u001b[0m         name=scope)\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36msummary_writer_function\u001b[0;34m(name, tensor, function, family)\u001b[0m\n\u001b[1;32m    763\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     op = smart_cond.smart_cond(\n\u001b[0;32m--> 765\u001b[0;31m         should_record_summaries(), record, _nothing, name=\"\")\n\u001b[0m\u001b[1;32m    766\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m     with ops.name_scope(name_scope), summary_op_util.summary_scope(\n\u001b[1;32m    757\u001b[0m         name, family, values=[tensor]) as (tag, scope):\n\u001b[0;32m--> 758\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(tag, scope)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         name=scope)\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msummary_writer_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary\u001b[0;34m(writer, step, tag, values, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m       return write_histogram_summary_eager_fallback(\n\u001b[0;32m--> 480\u001b[0;31m           writer, step, tag, values, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mwrite_histogram_summary_eager_fallback\u001b[0;34m(writer, step, tag, values, name, ctx)\u001b[0m\n\u001b[1;32m    497\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m   _result = _execute.execute(b\"WriteHistogramSummary\", 0, inputs=_inputs_flat,\n\u001b[0;32m--> 499\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    500\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mexecute_with_callbacks\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute_with_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m\"\"\"Monkey-patch to execute to enable execution callbacks.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquick_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repos/MachineLearningLecture/.env/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: dense_layer/kernel_0 [Op:WriteHistogramSummary]"
     ]
    }
   ],
   "source": [
    "!rm -rf /tmp/tfdbg2_logdir\n",
    "tf.debugging.experimental.enable_dump_debug_info('/tmp/tfdbg2_logdir', tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\n",
    "#run the pipeline:\n",
    "for i in pipeline.keys():\n",
    "    data,labels = pipeline[i]['data_loader']()\n",
    "\n",
    "    for p_fun in pipeline[i]['preprocessors']:\n",
    "        data = p_fun(data)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = pipeline[i]['splitter'](data,labels)\n",
    "    \n",
    "    kwargs  = pipeline[i]['model_args']\n",
    "    model = pipeline[i]['model'](data,labels,**kwargs)\n",
    "    OPTIMIZER = pipeline[i]['optimizer']\n",
    "    # Compiling the model.\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    log_dir = \"/tmp/tfdbg2_logdir/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Training the model.\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=BATCH_SIZE, \n",
    "              epochs=EPOCHS,\n",
    "              verbose=VERBOSE, \n",
    "              validation_split=VALIDATION_SPLIT,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n",
    "    #evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "    print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
